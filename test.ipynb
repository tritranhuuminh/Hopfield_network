## Perceptron
### Theoretical background

One of the most important models still in use today is the so-called perceptron, whose potential is defined as the weighted sum of incoming signals. If this internal potential of the neuron exceeds its threshold value (defined in our case by the symbol $ϑ$), the neuron is excited to a value of $1$. Otherwise, the neuron is inhibited, which is represented by a value of $0$. Mathematically, this process can be expressed using the signum function in this way:

$$
y = \text{Sgn}\left(\sum_{i=1}^n w_i x_i - \theta\right),
$$

where

$$
\text{Sgn}(x) = \begin{cases} 
1 & \text{if } x > 0, \\
0 & \text{if } x \leq 0.
\end{cases}
$$

By introducing a constant neuron at the input with an excitation state of $x_0 = 1$ and a connection to our neuron $w_0 = -θ$ the previous can be simplified as follows:

$$
y = \text{Sgn}\left(\sum_{i=0}^n w_i x_i\right).
$$

If we analyze the expression in the parentheses such that we set it equal to zero, we obtain the equation of a hyperplane (in the two-dimensional case represented by a line). Thus, using vector notation:
$$
\sum_{i=0}^n w_ix_i = \mathbf{W} \cdot \mathbf{X},
$$
$$
\mathbf{W} \cdot \mathbf{X} = 0.
$$

This plane divides the input space into two half-spaces (see figure). In other words, we are able to use the perceptron to distinguish two classes of inputs. One of them corresponds to an excitation equal to the value 1 and the other to inhibition of the neuron given the value 0. Values $x_1$ and $x_2$ are inputs from the interval $[0, 1]$.

<div style="text-align: center;">
    <img src="perceptron_recognition.png" alt="Perceptron Recognition" style="width: 50%;">
</div>

The question now is how to determine the values of the neuron's weights so that it can correctly recognize (assign to classes) the presented inputs. To achieve this, it is necessary to adapt our perceptron based on a training set through some algorithm. One of the most well-known principles is the adaptation (learning) of the neuron according to Hebb's rule.

**Perceptron learning (Hebb)**
1. Initialization of weights and threshold with random small numbers
   $$w_i(t), (0 \leq i \leq n) \text{ is the weight of input } i \text{ at time } t.$$

2. Presentation of the input and the desired output from the training set
   $$(x_0, x_1, \ldots, x_n) \rightarrow d(t).$$

3. Determination of the actual response
   $$y(t) = \text{Sgn}\left(\sum_{i=0}^n w_i(t)x_i(t)\right).$$

4. Weight Adaptation
   - If the output is correct: $w_i(t+1) = w_i(t)$,
   - If the output is 0 and should have been 1: $w_i(t+1) = w_i(t) + x_i(t)$,
   - If the output is 1 and should have been 0: $w_i(t+1) = w_i(t) - x_i(t)$.

This last step of the algorithm can be modified using a multiplicative factor, which can change the magnitude of changes in adapted weights as follows:

4. Weight adaptation with the learning rate
   - If the output is correct: $w_i(t+1) = w_i(t)$,
   - If the output is 0 and should have been 1: $w_i(t+1) = w_i(t) + \eta x_i(t)$,
   - If the output is 1 and should have been 0: $w_i(t+1) = w_i(t) - \eta x_i(t)$,
   
where $0 \leq \eta \leq 1$ is the learning rate affecting the adaptation process.

If an error of response $\Delta$ is introduced as defined by the difference between the desired and actual outputs:
$$
\Delta = d(t) - y(t),
$$

then we can generalize the weight adaptation by the following prescription:

4. Weight Adaptation with the error of response
$$
\Delta = d(t) - y(t),
$$
$$
w_i(t+1) = w_i(t) + \eta \Delta x_i(t).
$$




